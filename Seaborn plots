##################################
# Changes:
# !!!! Max features



#################################
# Add packages (replace first block with the pacakges here)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator
import matplotlib.ticker as ticker
import seaborn as sns
sns.set_theme(style="darkgrid")

################################
# Data exploration: max occurrence of words

plt.figure(figsize=(14, 8))
plt.yticks(fontsize=15)

g = sns.lineplot(feature_max.index, feature_max.values)
g.grid(axis = 'x')
g.text(11000 + 10, 471 + 10, "the: 471", color = 'r', fontsize = 15)
g.text(11126 + 10, 262 + 10, "to: 262", color = 'r', fontsize = 15)
g.text(7629 + 10 , 212 + 10, "of: 212", color = 'r', fontsize = 15)
g.text(810 + 10, 210 + 10, "and: 210", color = 'r', fontsize = 15)

g.set(xticklabels = [])
g.set_title("Maximum occurrence of each word", fontsize = 20, loc = "center")
g.set_xlabel("Words", fontsize = 15)
g.set_ylabel("Maximum occurrence", fontsize = 15)

g.axhline(y = 10, color = 'r', linestyle='--')
g.text(-400, 12, "10", color = 'r', fontsize = 15)
g.axhline(y = 50, color = 'r', linestyle='--')
g.text(-400, 52, "50", color = 'r', fontsize = 15)


##################################
# PCA plots
pca_ratio_sum = pd.Series(np.cumsum(pca.explained_variance_ratio_))
V = pd.DataFrame(data = pca_opt.components_.T, index = X_train.columns)

f, ax = plt.subplots(figsize=(16, 6.5))
f.tight_layout()
plt.subplots_adjust(wspace = 0.3, hspace = 0.6)

plt.subplot(2,1,1)
g = sns.lineplot(pca_ratio_sum.index, pca_ratio_sum.values)
g.set_xticklabels(g.get_xticklabels(), rotation=45)
g.xaxis.set_major_locator(ticker.MultipleLocator(25))
g.xaxis.set_major_formatter(ticker.ScalarFormatter())
g.set_title("Number of components needed to explain variance", 
            fontsize = 20, loc = "center")
g.set_xlabel("Number of components", fontsize = 15)
g.set_ylabel("cumulative explained variance", fontsize = 15)
g.axhline(y = 0.95, color = 'r', linestyle='-')
g.axvline(x = 310, ymax = 1, color = 'r', linestyle = "--")
g.text(315, 0.9, "n = 310", fontsize = 15)
g.text(10, 0.96, "95%", fontsize = 15)
plt.xlim(-5, 1000)
plt.ylim(0.60, None)

plt.subplot(2,2,3)
g = sns.barplot(V[0].sort_values(ascending = False).head(5).values,
               V[0].sort_values(ascending = False).head(5).index)
g.set_title("Top 5 words with the largest magitude in the first PC",
      fontsize = 20, loc = "center")
g.set_xlabel("Manitude of words",  fontsize = 15)
g.set_ylabel("Variable names",  fontsize = 15)
g.set_yticklabels(g.get_yticklabels(), fontsize = 15)

plt.subplot(2,2,4)
g = sns.barplot(V[1].sort_values(ascending = False).head(5).values,
               V[1].sort_values(ascending = False).head(5).index)
g.set_title("Top 5 words with the largest magitude in the second PC",
      fontsize = 20, loc = "center")
g.set_xlabel("Manitude of words",  fontsize = 15)
g.set_ylabel("Variable names",  fontsize = 15)
g.set_yticklabels(g.get_yticklabels(), fontsize = 15)

###############################
# Decision tree: importance feature
# !!! set ``ascending = False`` in the last line before plotting

# get importance and index for first ten words
dt_importances = dt_final.feature_importances_
dt_ind = np.argpartition(dt_importances, -10)[-10:]

# construct importance dataframe
dt_importance_df = pd.DataFrame(dt_importances[dt_ind])
dt_importance_df.set_axis(np.array(list(X_train_new))[dt_ind], axis = 0, inplace = True)
dt_importance_df.columns = ["importances"]
dt_importance_df = dt_importance_df.sort_values(by = ['importances'], ascending = False)

# plot: 
# !!! NEED TO DECIDE which words are important after re-running the codes and change the corlor pattern ``clrs`` accordingly
clrs = ['#d62728' if (x in ["image", "hillary", "michelle"]) else '#1f77b4' for x in dt_importance_df.index]

plt.figure(figsize = [12, 6.5])
g = sns.barplot(dt_importance_df.importances,
                dt_importance_df.index, 
                palette = clrs)
g.set_title("Decision Tree Feature importances", 
            fontsize = 20, loc = "center")
g.set_xlabel("Decrease in Gini impurity", fontsize = 15)
g.set_ylabel("Variable names", fontsize = 15)
g.set_yticklabels(g.get_yticklabels(), fontsize = 15)
g.text(0.65, 1, "0.67", fontsize = 17)
plt.show()

###########################
# Random forest (full-depth)
# Run permutations here ... (not included here)

# !!! Again, set ``ascending = False``
# construct importance dataframe
clf_importance_df = pd.DataFrame(clf_importances[clf_ind])
clf_importance_df.set_axis(np.array(list(X_train_new))[clf_ind], axis = 0, inplace = True)
clf_importance_df.columns = ['importances']
clf_importance_df = clf_importance_df.sort_values(by = ['importances'], ascending = False)

# plot
# !!! CHANGE the strings of interest here after re-running the codes
clrs = ['#d62728' if (x in ["image", "hillary", "michelle"]) else '#1f77b4' for x in clf_importance_df.index]

plt.figure(figsize = [12, 6.5])
g = sns.barplot(clf_importance_df.importances,
                clf_importance_df.index, 
                palette = clrs)
g.set_title("Random Forest(full depth) feature importances", 
            fontsize = 20, loc = "center")
g.set_xlabel("Decrease in Gini impurity", fontsize = 15)
g.set_ylabel("Variable names", fontsize = 15)
g.set_yticklabels(g.get_yticklabels(), fontsize = 15)

# !!! text annotation depending on the results
# g.text(0.65, 1, "0.67", fontsize = 17)
plt.show()

####################################
# Random forest (tune max depth)
# !!! Similar to above, after the permutations, set ``ascending = False``

# construct importance dataframe
rf_importance_df = pd.DataFrame(rf_importances[rf_ind])
rf_importance_df.set_axis(np.array(list(X_train_new))[rf_ind], axis = 0, inplace = True)
rf_importance_df.columns = ['importances']
rf_importance_df = rf_importance_df.sort_values(by = ['importances'], ascending = False)

# plot
# !!! CHANGE the strings of interest here after re-running the codes
clrs = ['#d62728' if (x in ["image", "hillary", "michelle"]) else '#1f77b4' for x in rf_importance_df.index]

plt.figure(figsize = [12, 6.5])
g = sns.barplot(rf_importance_df.importances,
                rf_importance_df.index, 
                palette = clrs)
g.set_title("Random Forest Feature Importances", 
            fontsize = 20, loc = "center")
g.set_xlabel("Decrease in Gini impurity", fontsize = 15)
g.set_ylabel("Variable names", fontsize = 15)
g.set_yticklabels(g.get_yticklabels(), fontsize = 15)

# !!! text annotation depending on the results
# g.text(0.65, 1, "0.67", fontsize = 17)
plt.show()

##################################
# AdaBoost important features
# !!! ``ascending = False``
# plot
# !!! CHANGE the strings of interest here after re-running the codes
clrs = ['#d62728' if (x in ["image", "hillary", "michelle"]) else '#1f77b4' for x in ada_importance_df.index]

plt.figure(figsize = [12, 6.5])
g = sns.barplot(ada_importance_df.importances,
                ada_importance_df.index, 
                palette = clrs)
g.set_title("Adaboost Feature Importances", 
            fontsize = 20, loc = "center")
g.set_xlabel("Decrease in Gini impurity", fontsize = 15)
g.set_ylabel("Variable names", fontsize = 15)
g.set_yticklabels(g.get_yticklabels(), fontsize = 15)

# !!! text annotation depending on the results
# g.text(0.65, 1, "0.67", fontsize = 17)
plt.show()

###################################
# AdaBoost
# ROC curve
plt.figure(figsize = [8, 5])
fhat_test = ada_final.predict_proba(X_test_new)
fpr_test, tpr_test, _ = roc_curve(y_test_new, fhat_test[:,1])
plt.plot(fpr_test, tpr_test)
plt.xlabel('False Positive Rate', fontsize = 15)
plt.ylabel('True Positive Rate', fontsize = 15)
plt.title('ROC curve', fontsize = 20, loc = "center")
plt.show()

##################################
# AdaBoost Error plot
ada_400 = AdaBoostClassifier(n_estimators = 400, algorithm = 'SAMME')
ada_400.fit(X_train_new, y_train_new)
ada_400_pred = ada_final.predict(X_test_new)
n_estimators = 400

ada_err = np.zeros((n_estimators,))
for i, y_pred in enumerate(ada_400.staged_predict(X_test_new)):
    ada_err[i] = zero_one_loss(y_pred, y_test_new)

ada_err_train = np.zeros((n_estimators,))
for i, y_pred in enumerate(ada_400.staged_predict(X_train_new)):
    ada_err_train[i] = zero_one_loss(y_pred, y_train_new)

# Plot
fig = plt.figure(figsize = (14, 8))
ax = fig.add_subplot(111)

sns.lineplot(
    np.arange(n_estimators) + 1,
    ada_err,
    label = 'Discrete AdaBoost Test Error',
    color = 'red',
    ax = ax
)
sns.lineplot(
    np.arange(n_estimators) + 1,
    ada_err_train,
    label = 'Discrete AdaBoost Train Error',
    color = 'blue',
    ax = ax
)

ax.set_ylim((0.0, 0.2))
ax.set_title("Discrete AdaBoost testing and training error", 
             fontsize = 20, loc = "center")
ax.set_xlabel('Number of estimators', fontsize = 15)
ax.set_ylabel('Error rate', fontsize = 15)

leg = ax.legend(loc = 'upper right', fancybox = True)
leg.get_frame().set_alpha(0.7)

plt.show()
